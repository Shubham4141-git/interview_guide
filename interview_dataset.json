{"id": "q01_good", "question": "Explain the difference between INNER JOIN, LEFT JOIN, and FULL OUTER JOIN.", "answer": "INNER JOIN returns only rows that match in both tables. LEFT JOIN returns all rows from the left table plus matching rows from the right, using NULLs when there’s no match. FULL OUTER JOIN returns all rows from both tables, matching where possible and filling NULLs where not.", "expected_score": 5, "expected_band": "good", "skill": "SQL joins", "notes": "Correctly defines each join and the NULL behavior."}
{"id": "q01_ok", "question": "Explain the difference between INNER JOIN, LEFT JOIN, and FULL OUTER JOIN.", "answer": "INNER JOIN gives matching rows from both tables. LEFT JOIN keeps all rows from the left table even if the right doesn’t match. FULL OUTER JOIN combines left and right joins.", "expected_score": 3, "expected_band": "ok", "skill": "SQL joins", "notes": "Mostly correct but vague on FULL OUTER details and NULL handling."}
{"id": "q01_bad", "question": "Explain the difference between INNER JOIN, LEFT JOIN, and FULL OUTER JOIN.", "answer": "INNER JOIN deletes duplicate rows, LEFT JOIN sorts the output, and FULL OUTER JOIN encrypts the results. They are mainly performance options.", "expected_score": 1, "expected_band": "bad", "skill": "SQL joins", "notes": "Incorrect and off-topic; joins don’t sort/delete/encrypt."}
{"id": "q02_good", "question": "What is an index in a database, and when can it hurt performance?", "answer": "An index is a structure (often a B-tree) that speeds up lookups by allowing the DB to find rows without scanning the whole table. It can hurt performance on heavy writes because inserts, updates, and deletes must update the index. Too many or low-selectivity indexes also increase storage and can degrade query plans.", "expected_score": 5, "expected_band": "good", "skill": "SQL indexing", "notes": "Explains purpose and key tradeoffs (writes, selectivity, storage)."}
{"id": "q02_ok", "question": "What is an index in a database, and when can it hurt performance?", "answer": "An index helps queries run faster on a column. Too many indexes take up space and can slow down updates.", "expected_score": 3, "expected_band": "ok", "skill": "SQL indexing", "notes": "Correct but missing selectivity and plan nuance."}
{"id": "q02_bad", "question": "What is an index in a database, and when can it hurt performance?", "answer": "An index is a backup copy used for recovery. It hurts performance only when the network is slow.", "expected_score": 1, "expected_band": "bad", "skill": "SQL indexing", "notes": "Confuses index with backup and gives irrelevant performance causes."}
{"id": "q03_good", "question": "What does ACID mean in databases?", "answer": "ACID stands for Atomicity, Consistency, Isolation, and Durability. Atomicity ensures all operations in a transaction succeed or none do; Consistency keeps database constraints valid; Isolation controls how concurrent transactions interact; Durability guarantees committed data survives crashes.", "expected_score": 5, "expected_band": "good", "skill": "Transactions (ACID)", "notes": "Accurate definitions for all four properties."}
{"id": "q03_ok", "question": "What does ACID mean in databases?", "answer": "ACID is Atomicity, Consistency, Isolation, and Durability. It basically means transactions are reliable and safe even with multiple users.", "expected_score": 3, "expected_band": "ok", "skill": "Transactions (ACID)", "notes": "Names correctly but explanations are too general."}
{"id": "q03_bad", "question": "What does ACID mean in databases?", "answer": "ACID is a query language like SQL that makes joins faster and compresses tables.", "expected_score": 1, "expected_band": "bad", "skill": "Transactions (ACID)", "notes": "Completely incorrect meaning."}
{"id": "q04_good", "question": "When would you use a window function, and how is it different from GROUP BY?", "answer": "Window functions compute values (like rank or running totals) across a set of rows while keeping each original row in the result. GROUP BY collapses rows into aggregated groups, reducing output granularity. Window functions let you mix detailed columns and analytic aggregates without losing row-level detail.", "expected_score": 5, "expected_band": "good", "skill": "SQL window functions", "notes": "Correct contrast: row preservation vs aggregation collapse."}
{"id": "q04_ok", "question": "When would you use a window function, and how is it different from GROUP BY?", "answer": "Window functions are used for ranking or moving averages. GROUP BY is for aggregations. Window functions are similar but you can still see more columns.", "expected_score": 3, "expected_band": "ok", "skill": "SQL window functions", "notes": "Mostly right but lacks clear explanation of partitions/windows."}
{"id": "q04_bad", "question": "When would you use a window function, and how is it different from GROUP BY?", "answer": "Window functions are for opening pop-up windows in SQL tools, and GROUP BY groups files on your desktop.", "expected_score": 1, "expected_band": "bad", "skill": "SQL window functions", "notes": "Off-topic and incorrect."}
{"id": "q05_good", "question": "Explain Python list vs tuple and when you’d choose each.", "answer": "Lists are mutable and support appending, removing, or modifying elements; tuples are immutable. Use a list when you need to change items, and a tuple when you want a fixed collection (like coordinates) or need a hashable key. Tuples signal immutability and can be slightly more memory-efficient.", "expected_score": 5, "expected_band": "good", "skill": "Python data structures", "notes": "Accurate differences and practical selection guidance."}
{"id": "q05_ok", "question": "Explain Python list vs tuple and when you’d choose each.", "answer": "A list is like an array you can change, while a tuple is fixed. I’d use lists for most work and tuples when I don’t want to modify the values.", "expected_score": 3, "expected_band": "ok", "skill": "Python data structures", "notes": "Correct core idea but lacks details (hashability, memory)."}
{"id": "q05_bad", "question": "Explain Python list vs tuple and when you’d choose each.", "answer": "Tuples are faster because they run in parallel threads, and lists are only for strings. I choose tuples whenever I need to sort numbers.", "expected_score": 1, "expected_band": "bad", "skill": "Python data structures", "notes": "Incorrect claims and misuse of concepts."}
{"id": "q06_good", "question": "What is a generator in Python and why use it?", "answer": "A generator yields items lazily using 'yield', producing one value at a time instead of building an entire list. It’s useful for large or infinite streams, pipelines, or reading files line by line. This reduces memory usage and can make code more responsive.", "expected_score": 5, "expected_band": "good", "skill": "Python generators", "notes": "Correct definition and benefits with practical examples."}
{"id": "q06_ok", "question": "What is a generator in Python and why use it?", "answer": "A generator returns values gradually instead of all at once. You use it when the dataset is big so you don’t store everything in memory.", "expected_score": 3, "expected_band": "ok", "skill": "Python generators", "notes": "Mostly correct but misses yield/iteration semantics and details."}
{"id": "q06_bad", "question": "What is a generator in Python and why use it?", "answer": "A generator is a tool that generates random numbers for encryption and makes code run at GPU speed.", "expected_score": 1, "expected_band": "bad", "skill": "Python generators", "notes": "Wrong concept and irrelevant claims."}
{"id": "q07_good", "question": "What is the Python GIL and what does it imply?", "answer": "The GIL (Global Interpreter Lock) in CPython ensures that only one thread executes Python bytecode at a time, which limits CPU-bound multithreaded speedups. I/O-bound programs can still benefit from threads because they spend time waiting. For CPU-bound parallelism you typically use multiprocessing, native extensions, or alternative runtimes.", "expected_score": 5, "expected_band": "good", "skill": "Python concurrency (GIL)", "notes": "Correct implications and practical workarounds."}
{"id": "q07_ok", "question": "What is the Python GIL and what does it imply?", "answer": "The GIL is a lock that affects threading in Python. It can make multithreading slower for some workloads, so people sometimes use multiprocessing instead.", "expected_score": 3, "expected_band": "ok", "skill": "Python concurrency (GIL)", "notes": "Generally correct but missing CPython/bytecode and I/O vs CPU nuance."}
{"id": "q07_bad", "question": "What is the Python GIL and what does it imply?", "answer": "GIL is a graphics library. It implies your code will always run faster with more threads.", "expected_score": 1, "expected_band": "bad", "skill": "Python concurrency (GIL)", "notes": "Completely incorrect definition and implication."}
{"id": "q08_good", "question": "How do you handle exceptions in Python, and when should you use finally?", "answer": "Use try/except to catch specific exceptions and handle or re-raise them, avoiding overly broad catches. A finally block runs regardless of whether an exception occurred, so it’s useful for cleanup tasks like closing files or releasing locks. Context managers via 'with' often replace manual finally blocks for resources.", "expected_score": 5, "expected_band": "good", "skill": "Python error handling", "notes": "Covers best practices, specificity, cleanup, and alternatives."}
{"id": "q08_ok", "question": "How do you handle exceptions in Python, and when should you use finally?", "answer": "You use try and except to catch errors. Finally is used when you always want some code to run, like closing something.", "expected_score": 3, "expected_band": "ok", "skill": "Python error handling", "notes": "Correct but lacks specificity and best-practice details."}
{"id": "q08_bad", "question": "How do you handle exceptions in Python, and when should you use finally?", "answer": "Exceptions are avoided by adding more print statements. 'finally' is used to end the program immediately after an error.", "expected_score": 1, "expected_band": "bad", "skill": "Python error handling", "notes": "Incorrect and misleading guidance."}
{"id": "q09_good", "question": "What is time complexity and how would you estimate it for a loop with a nested loop?", "answer": "Time complexity describes how runtime grows with input size, often expressed in Big-O notation. For nested loops, you multiply their sizes: an outer loop over n and an inner loop over n gives O(n^2). You focus on the dominant term and ignore constants for asymptotic growth.", "expected_score": 5, "expected_band": "good", "skill": "Algorithms (Big-O)", "notes": "Accurate explanation and correct estimation approach."}
{"id": "q09_ok", "question": "What is time complexity and how would you estimate it for a loop with a nested loop?", "answer": "Time complexity is how long an algorithm takes as input grows. A nested loop is usually n squared, like O(n^2), depending on how many times the loops run.", "expected_score": 3, "expected_band": "ok", "skill": "Algorithms (Big-O)", "notes": "Mostly correct but lacks clarity on differing loop bounds and dominant terms."}
{"id": "q09_bad", "question": "What is time complexity and how would you estimate it for a loop with a nested loop?", "answer": "Time complexity is the time on your laptop clock. Nested loops are always O(1) because the CPU is fast.", "expected_score": 1, "expected_band": "bad", "skill": "Algorithms (Big-O)", "notes": "Incorrect definition and wrong complexity claim."}
{"id": "q10_good", "question": "Explain bias vs variance in machine learning.", "answer": "Bias is error from overly simplistic assumptions that cause underfitting; variance is error from sensitivity to training data noise that leads to overfitting. High bias models perform poorly on both train and test, while high variance models do well on train but poorly on test. You trade off bias and variance using model complexity, regularization, and more data.", "expected_score": 5, "expected_band": "good", "skill": "ML fundamentals (bias-variance)", "notes": "Correct definitions, symptoms, and mitigation levers."}
{"id": "q10_ok", "question": "Explain bias vs variance in machine learning.", "answer": "Bias is when the model is too simple and variance is when it’s too complex. You want a balance so it generalizes better.", "expected_score": 3, "expected_band": "ok", "skill": "ML fundamentals (bias-variance)", "notes": "Right idea but too shallow and missing diagnostics/solutions."}
{"id": "q10_bad", "question": "Explain bias vs variance in machine learning.", "answer": "Bias is when the dataset is politically biased and variance is when labels change randomly. The fix is to always use bigger GPUs.", "expected_score": 1, "expected_band": "bad", "skill": "ML fundamentals (bias-variance)", "notes": "Mixes unrelated meaning and gives incorrect remedy."}
{"id": "q11_good", "question": "What is regularization and why does it help?", "answer": "Regularization adds a penalty to the loss to discourage overly complex models and reduce overfitting. L2 regularization adds a weight-squared penalty that shrinks weights smoothly, while L1 adds absolute values and can drive some weights to zero, acting like feature selection. This improves generalization by limiting model capacity relative to data.", "expected_score": 5, "expected_band": "good", "skill": "ML regularization (L1/L2)", "notes": "Correct purpose and distinguishes L1 vs L2 behavior."}
{"id": "q11_ok", "question": "What is regularization and why does it help?", "answer": "Regularization prevents overfitting by adding a penalty to the loss. It helps the model generalize better on new data.", "expected_score": 3, "expected_band": "ok", "skill": "ML regularization (L1/L2)", "notes": "Correct but missing key examples and how it affects parameters."}
{"id": "q11_bad", "question": "What is regularization and why does it help?", "answer": "Regularization means normalizing your features to between 0 and 1. It helps because the model becomes unbiased automatically.", "expected_score": 1, "expected_band": "bad", "skill": "ML regularization (L1/L2)", "notes": "Confuses regularization with normalization and makes false claim."}
{"id": "q12_good", "question": "What is cross-validation and when would you prefer it over a single train/test split?", "answer": "Cross-validation splits data into multiple folds (like k-fold), training on k−1 folds and validating on the remaining fold, repeating and averaging to estimate performance robustly. It’s preferred when data is limited or you want a less noisy estimate than a single split, and is useful for model selection and hyperparameter tuning.", "expected_score": 5, "expected_band": "good", "skill": "ML evaluation (cross-validation)", "notes": "Accurate definition and appropriate use cases."}
{"id": "q12_ok", "question": "What is cross-validation and when would you prefer it over a single train/test split?", "answer": "Cross-validation means splitting the data multiple times and averaging the results. I’d use it when I want a better estimate of accuracy than one split.", "expected_score": 3, "expected_band": "ok", "skill": "ML evaluation (cross-validation)", "notes": "Correct idea but lacks fold mechanics and tuning context."}
{"id": "q12_bad", "question": "What is cross-validation and when would you prefer it over a single train/test split?", "answer": "Cross-validation is when you validate across different programming languages like Python and Java. It’s always faster than a train/test split.", "expected_score": 1, "expected_band": "bad", "skill": "ML evaluation (cross-validation)", "notes": "Incorrect definition and false performance claim."}
{"id": "q13_good", "question": "What is data leakage? Give an example and how to prevent it.", "answer": "Data leakage occurs when information unavailable at prediction time leaks into training, inflating performance. Example: computing scaling or encoding using the entire dataset before splitting, or using future-derived features in a time-series problem. Prevent leakage by splitting first, fitting preprocessing on training data via pipelines, and using time-aware validation when needed.", "expected_score": 5, "expected_band": "good", "skill": "ML pitfalls (data leakage)", "notes": "Defines leakage, gives realistic examples, and prevention steps."}
{"id": "q13_ok", "question": "What is data leakage? Give an example and how to prevent it.", "answer": "Leakage is when the model accidentally sees test data during training. For example, if you mix train and test rows. You prevent it by keeping train and test separate.", "expected_score": 3, "expected_band": "ok", "skill": "ML pitfalls (data leakage)", "notes": "Partially correct but misses subtle leakage via preprocessing/future info."}
{"id": "q13_bad", "question": "What is data leakage? Give an example and how to prevent it.", "answer": "Data leakage is when the database has a memory leak. You prevent it by restarting the server daily.", "expected_score": 1, "expected_band": "bad", "skill": "ML pitfalls (data leakage)", "notes": "Confuses ML leakage with system memory leak."}
{"id": "q14_good", "question": "For an imbalanced classification problem, why can accuracy be misleading and what metrics would you use?", "answer": "With heavy imbalance, a model can get high accuracy by always predicting the majority class while failing on the minority. Metrics like precision, recall, F1-score, PR-AUC, and ROC-AUC better capture minority performance. I’d also look at the confusion matrix and choose thresholds based on costs.", "expected_score": 5, "expected_band": "good", "skill": "ML metrics (imbalanced data)", "notes": "Explains why accuracy fails and lists appropriate metrics and thresholding."}
{"id": "q14_ok", "question": "For an imbalanced classification problem, why can accuracy be misleading and what metrics would you use?", "answer": "Accuracy can look good even if the model misses the rare class. I’d use precision and recall, and maybe F1, to understand performance better.", "expected_score": 3, "expected_band": "ok", "skill": "ML metrics (imbalanced data)", "notes": "Good direction but missing PR-AUC/confusion matrix/threshold tradeoffs."}
{"id": "q14_bad", "question": "For an imbalanced classification problem, why can accuracy be misleading and what metrics would you use?", "answer": "Accuracy is always the best metric because it summarizes everything. For imbalance I’d only use mean squared error since it’s more scientific.", "expected_score": 1, "expected_band": "bad", "skill": "ML metrics (imbalanced data)", "notes": "Incorrect: accuracy can be misleading; MSE is not standard for classification here."}
{"id": "q15_good", "question": "Describe a typical feature engineering process for tabular data.", "answer": "I start with understanding the data: types, missing values, outliers, and the target. Then I create or transform features—scaling, log transforms, date and time aggregates, interaction terms—and encode categoricals with one-hot or target encoding carefully. I validate with a leakage-safe split and iterate using feature importance and error analysis.", "expected_score": 5, "expected_band": "good", "skill": "Feature engineering", "notes": "Covers data profiling, transformations, encoding, validation, and iteration."}
{"id": "q15_ok", "question": "Describe a typical feature engineering process for tabular data.", "answer": "I clean data, fill missing values, and convert categorical columns to numbers. Then I try some transformations and train a model to see if performance improves.", "expected_score": 3, "expected_band": "ok", "skill": "Feature engineering", "notes": "Reasonable but lacks methodology and leakage concerns."}
{"id": "q15_bad", "question": "Describe a typical feature engineering process for tabular data.", "answer": "Feature engineering just means renaming columns. You don’t need to do anything else because models learn everything automatically.", "expected_score": 1, "expected_band": "bad", "skill": "Feature engineering", "notes": "Incorrect: feature engineering is far more than renaming columns."}
{"id": "q16_good", "question": "Explain normalization vs denormalization in databases and when you’d use each.", "answer": "Normalization organizes data into tables to reduce redundancy and improve integrity, often through normal forms and foreign keys. It’s useful when you need consistency and transactional updates. Denormalization intentionally introduces redundancy by combining tables to optimize read performance at the cost of update complexity. It’s used in reporting and high-read workloads where joins are expensive.", "expected_score": 5, "expected_band": "good", "skill": "Database design (normalization)", "notes": "Accurate definition and trade-offs with examples."}
{"id": "q16_ok", "question": "Explain normalization vs denormalization in databases and when you’d use each.", "answer": "Normalization is splitting tables to avoid repetition, while denormalization is combining tables. Use normalization for integrity and denormalization for speed.", "expected_score": 3, "expected_band": "ok", "skill": "Database design (normalization)", "notes": "Good summary but lacks details on when to choose each."}
{"id": "q16_bad", "question": "Explain normalization vs denormalization in databases and when you’d use each.", "answer": "Normalization means formatting numbers between 0 and 1, and denormalization means storing images directly in the database.", "expected_score": 1, "expected_band": "bad", "skill": "Database design (normalization)", "notes": "Confuses database normalization with data scaling and is off-topic."}
{"id": "q17_good", "question": "Compare REST and SOAP APIs and highlight their main differences.", "answer": "REST is an architectural style using standard HTTP verbs and often JSON, focusing on stateless interactions and resource representations. SOAP is a protocol using XML envelopes with strict standards and optional WS-* extensions, offering built-in error handling and formal contracts. REST is generally simpler and lighter, while SOAP is heavier but can be better for enterprise scenarios requiring strict standards.", "expected_score": 5, "expected_band": "good", "skill": "APIs (REST vs SOAP)", "notes": "Describes differences in style, protocol, format, and typical use cases."}
{"id": "q17_ok", "question": "Compare REST and SOAP APIs and highlight their main differences.", "answer": "REST uses HTTP and JSON, and SOAP uses XML. REST is usually easier to use, and SOAP is more complex and verbose.", "expected_score": 3, "expected_band": "ok", "skill": "APIs (REST vs SOAP)", "notes": "Correct but missing depth on statelessness, standards, and contracts."}
{"id": "q17_bad", "question": "Compare REST and SOAP APIs and highlight their main differences.", "answer": "REST and SOAP are two programming languages. SOAP is only used for cleaning data, while REST is for sleeping the server.", "expected_score": 1, "expected_band": "bad", "skill": "APIs (REST vs SOAP)", "notes": "Incorrect: misidentifies what REST/SOAP are and gives nonsense uses."}
{"id": "q18_good", "question": "Explain the CAP theorem and its implications for distributed systems.", "answer": "The CAP theorem states that in a distributed data store you can only fully achieve two of Consistency, Availability, and Partition tolerance at the same time. Under network partition, a system must choose between consistency (all nodes agree) and availability (responds to requests). Designers choose trade-offs based on application needs, e.g., favoring availability in user-facing services.", "expected_score": 5, "expected_band": "good", "skill": "Distributed systems (CAP theorem)", "notes": "Accurate statement of CAP and design trade-offs."}
{"id": "q18_ok", "question": "Explain the CAP theorem and its implications for distributed systems.", "answer": "CAP says you can’t have everything in a distributed database. You usually choose two of consistency, availability, or partition tolerance.", "expected_score": 3, "expected_band": "ok", "skill": "Distributed systems (CAP theorem)", "notes": "Correct idea but lacks detail on partition tolerance and design choices."}
{"id": "q18_bad", "question": "Explain the CAP theorem and its implications for distributed systems.", "answer": "The CAP theorem is about capacity planning for servers. It says you need more RAM to avoid partitions.", "expected_score": 1, "expected_band": "bad", "skill": "Distributed systems (CAP theorem)", "notes": "Incorrect: confuses CAP with hardware capacity planning."}
{"id": "q19_good", "question": "In designing a scalable web application, what are some approaches to handle high read traffic?", "answer": "To handle high read traffic, use caching layers (e.g., CDN, reverse proxies, in-memory caches like Redis) to serve frequently accessed data, employ database replicas or read replicas to distribute load, and denormalize or precompute results when appropriate. You can also use horizontal scaling and load balancers to distribute requests across servers.", "expected_score": 5, "expected_band": "good", "skill": "System design (scalability)", "notes": "Mentions caching, replicas, scaling, and precomputation."}
{"id": "q19_ok", "question": "In designing a scalable web application, what are some approaches to handle high read traffic?", "answer": "I would add more servers and maybe a cache. Replicating the database can also help so reads don’t all hit one machine.", "expected_score": 3, "expected_band": "ok", "skill": "System design (scalability)", "notes": "Partial: mentions adding servers and caching but not much detail."}
{"id": "q19_bad", "question": "In designing a scalable web application, what are some approaches to handle high read traffic?", "answer": "You should disable logging and comments to reduce traffic because that lowers the data the user sees.", "expected_score": 1, "expected_band": "bad", "skill": "System design (scalability)", "notes": "Incorrect: disabling features doesn’t address read scalability."}
{"id": "q20_good", "question": "What is MapReduce and how does it differ from Apache Spark?", "answer": "MapReduce is a programming model for processing large datasets across clusters by splitting work into map and reduce tasks, persisting intermediate results to disk, and is part of Hadoop. Spark is an in-memory distributed engine that offers similar map/reduce primitives but supports rich APIs (SQL, streaming, machine learning) and keeps data in memory for faster iterative processing. Spark generally offers better performance and ease of use.", "expected_score": 5, "expected_band": "good", "skill": "Big data processing (MapReduce vs Spark)", "notes": "Explains both systems, their models, and differences in storage and features."}
{"id": "q20_ok", "question": "What is MapReduce and how does it differ from Apache Spark?", "answer": "MapReduce splits jobs into map and reduce steps on a cluster. Spark is faster because it keeps data in memory and has more libraries.", "expected_score": 3, "expected_band": "ok", "skill": "Big data processing (MapReduce vs Spark)", "notes": "Mostly correct but lacks mention of Hadoop and broader API support."}
{"id": "q20_bad", "question": "What is MapReduce and how does it differ from Apache Spark?", "answer": "MapReduce is a storage system like a hard drive, and Spark is a graphics engine for game development.", "expected_score": 1, "expected_band": "bad", "skill": "Big data processing (MapReduce vs Spark)", "notes": "Incorrect: misidentifies what MapReduce and Spark are."}
